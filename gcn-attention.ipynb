{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Convolutional Prediction of Protein Interactions in Yeast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(This demo is a part of [Deep Learning for Network Biology](http://snap.stanford.edu/deepnetbio-ismb/) tutorial.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we demonstrate the utility of deep learning methods for an important prediction problem on biological graphs. In particular, we consider the problem of predicting [protein-protein interactions](https://en.wikipedia.org/wiki/Protein%E2%80%93protein_interaction) (PPIs). \n",
    "\n",
    "Protein-protein interactions (PPIs) are essential to almost every process in a cell. Understanding PPIs is crucial for understanding cell physiology in normal and disease states. Furthermore, knowledge of PPIs can be used:\n",
    "* for drug development, since drugs can affect PPIs,\n",
    "* to assign roles (i.e., protein functions) to uncharacterized proteins,\n",
    "* to characterize the relationships between proteins that form multi-molecular complexes, such as the proteasome.\n",
    "\n",
    "We represent the totality of PPIs that happen in a cell, an organism or a specific biological context with a [protein-protein interaction network](https://www.nature.com/subjects/protein-protein-interaction-networks). These networks are mathematical representations of all physical contacts between proteins in the cell.\n",
    "\n",
    "The development of large-scale PPI screening techniques, especially [high-throughput affinity purification combined with mass-spectrometry](https://doi.org/10.1016/j.tibtech.2016.02.014) and the [yeast two-hybrid assay](https://en.wikipedia.org/wiki/Two-hybrid_screening), has caused an explosion in the amount of PPI data and the construction of ever more complex and complete interaction networks. For example, the figure below is a graphical representation of three different types of protein-protein interaction networks in [yeast *S. cerevisiae*](https://en.wikipedia.org/wiki/Saccharomyces_cerevisiae). The structure of the binary interaction network is obviously different from the structure of the co-complex interaction network. The network structure of the literature-curated dataset resembles that of the co-complex dataset, even though the literature-curated datasets are reported to contain mostly binary interactions.\n",
    "\n",
    "However, current knowledge of protein-protein interaction networks is both [*incomplete* and *noisy*](https://doi.org/10.1016/j.cell.2014.10.050), as PPI screening techniques are limited in how many true interactions they can detect. Furthermore, PPI screening techniques often have high false positive and negative rates. These limitations present a great opportunity for computational methods to predict protein-protein interactions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"yeast_interactome.png\" alt=\"Yeast protein-protein interaction networks\" style=\"width: 800px\"/>\n",
    "</center>\n",
    "\n",
    "Image taken from: http://interactome.dfci.harvard.edu/S_cerevisiae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we take the yeast protein-protein interaction network ([download the preprocessed network](http://snap.stanford.edu/deepnetbio-ismb/ipynb/yeast.edgelist)) and use the network to build a model for predicting new protein-protein interactions. We formulate this prediction task as **a link prediction problem on unweighted and undirected networks** and use a graph convolutional neural network to solve the task.\n",
    "\n",
    "Note that link prediction is a standard prediction task in network biology, and thus this example can be easily extended and adapted to many other biomedical applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolutional Neural Network (GCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we give a complete [Tensorflow](https://www.tensorflow.org/) implementation of a two-layer graph convolutional neural network (GCN) for link prediction. We closely follow the GCN formulation as presented in [Kipf et al., ICLR 2017](https://arxiv.org/pdf/1609.02907.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fujitsuka/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import gc\n",
    "\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Settings\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 20, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 32, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_integer('hidden2', 16, 'Number of units in hidden layer 2.')\n",
    "flags.DEFINE_integer('hidden3', 16, 'Number of units in hidden layer 3.')\n",
    "flags.DEFINE_integer('hidden4', 16, 'Number of units in hidden layer 4.')\n",
    "flags.DEFINE_float('dropout', 0.1, 'Dropout rate (1 - keep probability).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(datapath):\n",
    "    \"\"\"\n",
    "    edgelistデータを読み込み，　sp.sparse型の隣接行列を返す\n",
    "    \"\"\"\n",
    "    g = nx.read_edgelist(datapath)\n",
    "    adj = nx.adjacency_matrix(g, nodelist=sorted(g.nodes()))\n",
    "    return adj.T\n",
    "\n",
    "\n",
    "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = tf.random_uniform(\n",
    "        [input_dim, output_dim], minval=-init_range,\n",
    "        maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
    "    noise_shape = [num_nonzero_elems]\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1. / keep_prob)\n",
    "\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"\n",
    "    sp.sparse行列を，　値を持つcoords，　値の中身, 本来の行列の形のtupleに変換\n",
    "    \"\"\"\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    \"\"\"\n",
    "    sp.sparse型の隣接行列を，　次数行列で正規化して，　coords, values, shape のtupleに変換\n",
    "    \"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "\n",
    "def construct_feed_dict(adj_normalized, adj, features, placeholders):\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['adj']: adj_normalized})\n",
    "    feed_dict.update({placeholders['adj_orig']: adj})\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def mask_test_edges(adj):\n",
    "    # Function to build test set with 2% positive links (全体の1/50のリンク)\n",
    "    # ここでは，　2%の正解に合わせて，　同じ数の不正解データも作っておく （精度を求める際に使用）\n",
    "    \n",
    "    # Remove diagonal elements\n",
    "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "\n",
    "    adj_triu = sp.triu(adj)\n",
    "    adj_tuple = sparse_to_tuple(adj_triu)\n",
    "    edges = adj_tuple[0]\n",
    "    edges_all = sparse_to_tuple(adj)[0]\n",
    "    num_test = int(np.floor(edges.shape[0] / 50.))\n",
    "    num_val = int(np.floor(edges.shape[0] / 50.))\n",
    "    \n",
    "    # エッジのidをシャッフルして，　訓練用, 検証用と，テスト用のエッジに分ける\n",
    "    all_edge_idx = [i for i in range(edges.shape[0])]\n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    val_edge_idx = all_edge_idx[:num_val]\n",
    "    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
    "    test_edges = edges[test_edge_idx]\n",
    "    val_edges = edges[val_edge_idx]\n",
    "    train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)  # 配列edgesから，　テストと検証用のエッジを除く\n",
    "\n",
    "    def ismember(a, b):\n",
    "        rows_close = np.all((a - b[:, None]) == 0, axis=-1)\n",
    "        return np.any(rows_close)\n",
    "\n",
    "    test_edges_false = []\n",
    "    while len(test_edges_false) < len(test_edges):\n",
    "        n_rnd = len(test_edges) - len(test_edges_false) \n",
    "        rnd = np.random.randint(0, adj.shape[0], size=2 * n_rnd)  # 2*n_rnd 個のリスト（各要素は，　ランダムに選ばれたノードの番号）\n",
    "        idxs_i = rnd[:n_rnd]  # 　半分を i と j それぞれに分ける                                      \n",
    "        idxs_j = rnd[n_rnd:]\n",
    "        for i in range(n_rnd):\n",
    "            idx_i = idxs_i[i]\n",
    "            idx_j = idxs_j[i]\n",
    "            if idx_i == idx_j:\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], edges_all):\n",
    "                continue\n",
    "            if test_edges_false:  # リストが空でないなら，　True\n",
    "                if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
    "                    continue\n",
    "                if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
    "                    continue\n",
    "            test_edges_false.append([idx_i, idx_j])  # 上記の条件を満たさない場合だけ，　test_edges_falseに加える\n",
    "\n",
    "    val_edges_false = []\n",
    "    while len(val_edges_false) < len(val_edges):\n",
    "        n_rnd = len(val_edges) - len(val_edges_false)\n",
    "        rnd = np.random.randint(0, adj.shape[0], size=2 * n_rnd)\n",
    "        idxs_i = rnd[:n_rnd]                                        \n",
    "        idxs_j = rnd[n_rnd:]\n",
    "        for i in range(n_rnd):\n",
    "            idx_i = idxs_i[i]\n",
    "            idx_j = idxs_j[i]\n",
    "            if idx_i == idx_j:\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], train_edges):\n",
    "                continue\n",
    "            if ismember([idx_j, idx_i], train_edges):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], val_edges):\n",
    "                continue\n",
    "            if ismember([idx_j, idx_i], val_edges):\n",
    "                continue\n",
    "            if val_edges_false:\n",
    "                if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
    "                    continue\n",
    "                if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
    "                    continue\n",
    "            val_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    # Re-build adj matrix\n",
    "    data = np.ones(train_edges.shape[0])\n",
    "    adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
    "    adj_train = adj_train + adj_train.T\n",
    "\n",
    "    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false\n",
    "\n",
    "\n",
    "def get_roc_score(edges_pos, edges_neg):\n",
    "    \"\"\"\n",
    "    評価用の，　正解エッジのリストと，　不正解エッジのリストを渡す\n",
    "    \"\"\"\n",
    "    feed_dict.update({placeholders['dropout']: 0})  # 評価なので，　dropoutは0に設定　　\n",
    "    emb = sess.run(model.embeddings, feed_dict=feed_dict)\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    adj_rec = np.dot(emb, emb.T)  # embは(N, F)行列ゆえ，　左のように積を取ると，　(N, N)で(i, j)成分はノード i と j の類似度を表す\n",
    "    \n",
    "    # positive edge と negative edge の2つで別々に評価\n",
    "    preds = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds))])\n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return roc_score, ap_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Convolutional Layers for our GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GraphConvolution():\n",
    "    \"\"\"Basic graph convolution layer for undirected graph without edge labels.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, adj, name, dropout=0., act=tf.nn.relu):\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        self.issparse = False\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name='weights')\n",
    "        self.dropout = dropout\n",
    "        self.adj = adj\n",
    "        self.act = act\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):        \n",
    "            x = inputs\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "            x = tf.matmul(x, self.vars['weights'])\n",
    "            x = tf.sparse_tensor_dense_matmul(self.adj, x)\n",
    "            outputs = self.act(x)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class GraphConvolutionSparse():\n",
    "    \"\"\"Graph convolution layer for sparse inputs.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, adj, features_nonzero, name, dropout=0., act=tf.nn.relu):\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        self.issparse = False\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name='weights')\n",
    "        self.dropout = dropout\n",
    "        self.adj = adj\n",
    "        self.act = act\n",
    "        self.issparse = True\n",
    "        self.features_nonzero = features_nonzero\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            x = inputs\n",
    "            x = dropout_sparse(x, 1-self.dropout, self.features_nonzero)\n",
    "            x = tf.sparse_tensor_dense_matmul(x, self.vars['weights'])\n",
    "            x = tf.sparse_tensor_dense_matmul(self.adj, x)\n",
    "            outputs = self.act(x)\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "class InnerProductDecoder():\n",
    "    \"\"\"Decoder model layer for link prediction.\"\"\"\n",
    "    def __init__(self, input_dim, name, dropout=0., act=tf.nn.sigmoid):\n",
    "        self.name = name\n",
    "        self.issparse = False\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            inputs = tf.nn.dropout(inputs, 1-self.dropout)\n",
    "            x = tf.transpose(inputs)\n",
    "            x = tf.matmul(inputs, x)  # 特徴ベクトルの内積，　(N, N)で各成分がノードとノードの類似度を表す\n",
    "            x = tf.reshape(x, [-1])\n",
    "            outputs = self.act(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GraphConvolution_Attention():\n",
    "    \"\"\"\n",
    "    attentionつき\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, adj, name, dropout=0., act=tf.nn.relu):\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        self.issparse = False\n",
    "        self.dropout = dropout\n",
    "        self.adj = adj\n",
    "        self.act = act\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name='weights')\n",
    "            self.vars['attention_self'] = weight_variable_glorot(output_dim, 1, name=\"attention_self\")\n",
    "            self.vars['attention_neigh'] = weight_variable_glorot(output_dim, 1, name=\"attention_neigh\")\n",
    "\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            x = inputs\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "            x = tf.matmul(x, self.vars['weights'])\n",
    "\n",
    "            # XWa = XWa_{self} + XWa_{neigh} と分ける\n",
    "            att_self = tf.matmul(x, self.vars['attention_self'])  # (N,1)\n",
    "            att_neigh = tf.matmul(x, self.vars['attention_neigh'])  # (N,1)\n",
    "            att = tf.add(att_self, tf.transpose(att_neigh))  # (N,1) + (1,N)　→ (N,N)にbroadcastされるのを利用\n",
    "            att = self.adj.__mul__(att)  # 隣接行列と要素積を取ったもの(__mul__でsparse型との要素積が可能, 結果はsparseとなる)\n",
    "            att = tf.SparseTensor(indices=att.indices, values=tf.nn.leaky_relu(att.values, alpha=0.2), dense_shape=att.dense_shape)\n",
    "            att = tf.sparse_softmax(att)\n",
    "            \n",
    "            x = tf.sparse_tensor_dense_matmul(att, x)\n",
    "            #x = tf.matmul(att, x)\n",
    "            outputs = self.act(x)\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "class GraphConvolutionSparse_Attention():\n",
    "    \"\"\"Graph convolution layer for sparse inputs.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, adj, features_nonzero, name, dropout=0., act=tf.nn.relu):\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        self.issparse = False\n",
    "        self.dropout = dropout\n",
    "        self.adj = adj\n",
    "        self.act = act\n",
    "        self.issparse = True\n",
    "        self.features_nonzero = features_nonzero\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name='weights')\n",
    "            self.vars['attention_self'] = weight_variable_glorot(output_dim, 1, name=\"attention_self\")\n",
    "            self.vars['attention_neigh'] = weight_variable_glorot(output_dim, 1, name=\"attention_neigh\")\n",
    "    \n",
    "    #def attention(self, X):\n",
    "        # XWの計算\n",
    "        #X_linear_tr = tf.matmul(X, self.vars['weights'])\n",
    "        # XWa = XWa_{self} + XWa_{neigh} と分ける\n",
    "        #att_self = tf.matmul(X_linear_tr, self.vars['attention_self'])  # (N,1)\n",
    "        #att_neigh = tf.matmul(X_linear_tr, self.vars['attention_neigh'])  # (N,1)\n",
    "        #att = tf.add(att_self, tf.transpose(att_neigh))  # (N,1) + (1,N)　→ (N,N)にbroadcastされるのを利用\n",
    "\n",
    "        #del X_linear_tr\n",
    "        #gc.collect()\n",
    "\n",
    "        #return self.adj.__mul__(att)  # 隣接行列と要素積を取ったもの(__mul__でsparse型との要素積が可能)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            x = inputs\n",
    "            x = dropout_sparse(x, 1-self.dropout, self.features_nonzero)\n",
    "            x = tf.sparse_tensor_dense_matmul(x, self.vars['weights'])  # この結果は， denseとなる\n",
    "            \n",
    "            # XWa = XWa_{self} + XWa_{neigh} と分ける\n",
    "            att_self = tf.matmul(x, self.vars['attention_self'])  # (N,1)\n",
    "            att_neigh = tf.matmul(x, self.vars['attention_neigh'])  # (N,1)\n",
    "            \n",
    "            # SparseTensor型で，　attentionの重みを作成\n",
    "            # 注意として，　placeholderは入るまで分からないため，　型とか値とかは，　この時点では，　？ なので，　例えば，　\n",
    "            #  att = tf.SparseTensor(indices=self.adj.indices, values=[att_self[self.adj.indices[i][0]]+att_neigh[self.adj.indices[i][1]] for i in range(self.adj.indices[0].value)], dense_shape=self.adj.dense_shape)\n",
    "            # 上記は，　self.adj.indices[0].valueがNoneゆえ，　ループが回らなく，　エラーとなる\n",
    "            \n",
    "            # (N,N)と(N,1)の要素積は，　(N,1)を(N,N)の列方向に各列に掛けていったものになる\n",
    "            # (N,N)と(1,N)の要素積は，　(1,N)を(N,N)の行方向に各行に掛けていったものになる\n",
    "            \n",
    "            att_1 = self.adj.__mul__(tf.nn.leaky_relu(att_self, alpha=0.2))\n",
    "            att_2 = self.adj.__mul__(tf.transpose(tf.nn.leaky_relu(att_neigh, alpha=0.2)))\n",
    "            att = tf.sparse_add(att_1, att_2)\n",
    "            \n",
    "            del att_1,   att_2\n",
    "            gc.collect()\n",
    "            \n",
    "            #att = tf.add(att_self, tf.transpose(att_neigh))  # (N,1) + (1,N)　→ (N,N)にbroadcastされるのを利用\n",
    "            #att = self.adj.__mul__(att)  # 隣接行列と要素積を取ったもの(adjはsparseゆえ，　__mul__でsparse型との要素積， 結果はsparseとなる)\n",
    "            #att = tf.SparseTensor(indices=att.indices, values=tf.nn.leaky_relu(att.values), dense_shape=att.dense_shape)\n",
    "            \n",
    "            att = tf.sparse_softmax(att)\n",
    "            \n",
    "            x = tf.sparse_tensor_dense_matmul(att, x)\n",
    "            outputs = self.act(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the Architecture of our GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GCNModel():\n",
    "    def __init__(self, placeholders, num_features, features_nonzero, name):\n",
    "        self.name = name\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = num_features\n",
    "        self.features_nonzero = features_nonzero\n",
    "        self.adj = placeholders['adj']\n",
    "        self.dropout = placeholders['dropout']\n",
    "        self.vars = {}\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.vars['attention_layer'] = tf.Variable([1,1,1], dtype=tf.float32, name=\"attention_layer\")\n",
    "            self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.hidden1 = GraphConvolutionSparse_Attention(\n",
    "            name='gcn_sparse_layer',\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=FLAGS.hidden1,\n",
    "            adj=self.adj,\n",
    "            features_nonzero=self.features_nonzero,\n",
    "            act=tf.nn.relu,\n",
    "            dropout=self.dropout)(self.inputs)\n",
    "\n",
    "        self.hidden2 = GraphConvolution_Attention(\n",
    "            name='gcn_dense_layer_1',\n",
    "            input_dim=FLAGS.hidden1,\n",
    "            output_dim=FLAGS.hidden2,\n",
    "            adj=self.adj,\n",
    "            act=lambda x: x,\n",
    "            dropout=self.dropout)(self.hidden1)\n",
    "        \n",
    "        self.hidden3 = GraphConvolution_Attention(\n",
    "            name='gcn_dense_layer_2',\n",
    "            input_dim=FLAGS.hidden2,\n",
    "            output_dim=FLAGS.hidden3,\n",
    "            adj=self.adj,\n",
    "            act=lambda x: x,\n",
    "            dropout=self.dropout)(self.hidden2)\n",
    "        \n",
    "        self.hidden4 = GraphConvolution_Attention(\n",
    "            name='gcn_dense_layer_3',\n",
    "            input_dim=FLAGS.hidden3,\n",
    "            output_dim=FLAGS.hidden4,\n",
    "            adj=self.adj,\n",
    "            act=lambda x: x,\n",
    "            dropout=self.dropout)(self.hidden3)\n",
    "        \n",
    "        weight = tf.nn.softmax(self.vars['attention_layer'] )\n",
    "        self.embeddings = self.vars['attention_layer'][0]*self.hidden2 + self.vars['attention_layer'][1]*self.hidden3 + self.vars['attention_layer'][2]*self.hidden4\n",
    "\n",
    "        self.reconstructions = InnerProductDecoder(\n",
    "            name='gcn_decoder',\n",
    "            input_dim=FLAGS.hidden4, \n",
    "            act=lambda x: x)(self.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the GCN Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, preds, labels, num_nodes, num_edges):\n",
    "        \n",
    "        # 重み付き交差エントロピー関数の重み\n",
    "        # A value pos_weights > 1 decreases the false negative count, hence increasing the recall. \n",
    "        # Conversely setting pos_weights < 1 decreases the false positive count and increases the precision.\n",
    "        pos_weight = float(num_nodes**2 - num_edges) / num_edges\n",
    "        norm = num_nodes**2 / float((num_nodes**2 - num_edges) * 2)\n",
    "        \n",
    "        preds_sub = preds\n",
    "        labels_sub = labels\n",
    "\n",
    "        self.cost = norm * tf.reduce_mean(\n",
    "            tf.nn.weighted_cross_entropy_with_logits(\n",
    "                logits=preds_sub, targets=labels_sub, pos_weight=pos_weight))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)  # Adam Optimizer\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.cost)\n",
    "        self.grads_vars = self.optimizer.compute_gradients(self.cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the GCN Model and Evaluate its Accuracy on a Test Set of Protein-Protein Interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a training set of protein-protein interactions in yeast *S. cerevisiae*, our goal is to take these interactions and train a GCN model that can predict new protein-protein interactions. That is, we would like to predict new edges in the yeast protein interaction network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adj = load_data('data/yeast.edgelist')\n",
    "num_nodes = adj.shape[0]\n",
    "num_edges = adj.sum()  # 今，　重みなしグラフを考えているので，　全て足すとエッジ数になる\n",
    "\n",
    "# featureless\n",
    "# 現状，　featuresがないので，　単位行列（one-hotベクトル）を入れる\n",
    "features = sparse_to_tuple(sp.identity(num_nodes))\n",
    "num_features = features[2][1]\n",
    "features_nonzero = features[1].shape[0]  # [1]成分は，　ノンゼロvalues\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "adj_norm = preprocess_graph(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define placeholders\n",
    "# 現状，　featureはないので，　sp.sparse表現でone-hotベクトルを入れているが，　ここは変更する（一般にsparse表現ではない）\n",
    "placeholders = {\n",
    "    'features': tf.sparse_placeholder(tf.float32),\n",
    "    'adj': tf.sparse_placeholder(tf.float32),\n",
    "    'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=())\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = GCNModel(placeholders, num_features, features_nonzero, name='yeast_gcn')\n",
    "\n",
    "# Create optimizer\n",
    "# adj = adj_trainと上で代入しており，　訓練データに関してoptimizeするように設定\n",
    "with tf.name_scope('optimizer'):\n",
    "    opt = Optimizer(\n",
    "        preds=model.reconstructions,\n",
    "        labels=tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices=False), [-1]),\n",
    "        num_nodes=num_nodes,\n",
    "        num_edges=num_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 0.71085 val_roc= 0.87384 val_ap= 0.86487 time= 22.08053\n",
      "Epoch: 0002 train_loss= 0.68903 val_roc= 0.87541 val_ap= 0.86599 time= 21.28690\n",
      "Epoch: 0003 train_loss= 0.63423 val_roc= 0.87597 val_ap= 0.86633 time= 22.02609\n",
      "Epoch: 0004 train_loss= 0.61024 val_roc= 0.87627 val_ap= 0.86653 time= 21.93498\n",
      "Epoch: 0005 train_loss= 0.63731 val_roc= 0.87638 val_ap= 0.86661 time= 22.64135\n",
      "Epoch: 0006 train_loss= 0.61285 val_roc= 0.87645 val_ap= 0.86667 time= 21.95668\n",
      "Epoch: 0007 train_loss= 0.60488 val_roc= 0.87658 val_ap= 0.86676 time= 22.39017\n",
      "Epoch: 0008 train_loss= 0.61457 val_roc= 0.87675 val_ap= 0.86685 time= 21.57474\n",
      "Epoch: 0009 train_loss= 0.61865 val_roc= 0.87692 val_ap= 0.86695 time= 22.31925\n",
      "Epoch: 0010 train_loss= 0.61470 val_roc= 0.87708 val_ap= 0.86703 time= 22.68117\n",
      "Epoch: 0011 train_loss= 0.60752 val_roc= 0.87720 val_ap= 0.86709 time= 22.03820\n",
      "Epoch: 0012 train_loss= 0.60314 val_roc= 0.87731 val_ap= 0.86714 time= 22.30449\n",
      "Epoch: 0013 train_loss= 0.60730 val_roc= 0.87741 val_ap= 0.86718 time= 22.37422\n",
      "Epoch: 0014 train_loss= 0.61100 val_roc= 0.87750 val_ap= 0.86722 time= 21.88855\n",
      "Epoch: 0015 train_loss= 0.61026 val_roc= 0.87757 val_ap= 0.86725 time= 22.30909\n",
      "Epoch: 0016 train_loss= 0.60377 val_roc= 0.87765 val_ap= 0.86727 time= 22.40306\n",
      "Epoch: 0017 train_loss= 0.60263 val_roc= 0.87770 val_ap= 0.86728 time= 21.70535\n",
      "Epoch: 0018 train_loss= 0.60481 val_roc= 0.87771 val_ap= 0.86727 time= 21.62410\n",
      "Epoch: 0019 train_loss= 0.60669 val_roc= 0.87771 val_ap= 0.86726 time= 21.38552\n",
      "Epoch: 0020 train_loss= 0.60561 val_roc= 0.87772 val_ap= 0.86725 time= 21.48005\n",
      "Optimization Finished!\n",
      "Test ROC score: 0.87882\n",
      "Test AP score: 0.86798\n"
     ]
    }
   ],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "adj_label = sparse_to_tuple(adj_label)\n",
    "\n",
    "# Train model\n",
    "for epoch in range(FLAGS.epochs):\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    # placeholdersに代入する辞書作り\n",
    "    feed_dict = construct_feed_dict(adj_norm, adj_label, features, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "    # One update of parameter matrices\n",
    "    _, avg_cost = sess.run([opt.opt_op, opt.cost], feed_dict=feed_dict)  # optimizeして，　コスト関数を出力\n",
    "    # Performance on validation set\n",
    "    roc_curr, ap_curr = get_roc_score(val_edges, val_edges_false)  # ただ各エポックで精度を見るためだけのもの\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \n",
    "          \"train_loss=\", \"{:.5f}\".format(avg_cost),\n",
    "          \"val_roc=\", \"{:.5f}\".format(roc_curr),\n",
    "          \"val_ap=\", \"{:.5f}\".format(ap_curr),\n",
    "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "print('Optimization Finished!')\n",
    "\n",
    "roc_score, ap_score = get_roc_score(test_edges, test_edges_false)\n",
    "print('Test ROC score: {:.5f}'.format(roc_score))\n",
    "print('Test AP score: {:.5f}'.format(ap_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
